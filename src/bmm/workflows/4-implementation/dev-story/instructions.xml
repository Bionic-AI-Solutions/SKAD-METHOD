<workflow>
  <critical>The workflow execution engine is governed by: {project-root}/_bmad/core/tasks/workflow.xml</critical>
  <critical>You MUST have already loaded and processed: {installed_path}/workflow.yaml</critical>
  <critical>Communicate all responses in {communication_language} and language MUST be tailored to {user_skill_level}</critical>
  <critical>Generate all documents in {document_output_language}</critical>
  <critical>Only modify the story file in these areas: Tasks/Subtasks checkboxes, Dev Agent Record (Debug Log, Completion Notes), File List,
    Change Log, and Status</critical>
  <critical>Execute ALL steps in exact order; do NOT skip steps</critical>
  <critical>Absolutely DO NOT stop because of "milestones", "significant progress", or "session boundaries". Continue in a single execution
    until the story is COMPLETE (all ACs satisfied and all tasks/subtasks checked) UNLESS a HALT condition is triggered or the USER gives
    other instruction.</critical>
  <critical>Do NOT schedule a "next session" or request review pauses unless a HALT condition applies. Only Step 6 decides completion.</critical>
  <critical>User skill level ({user_skill_level}) affects conversation style ONLY, not code updates.</critical>

  <step n="1" goal="Find next ready story and load it" tag="sprint-status">
    <check if="{{story_path}} is provided">
      <action>Use {{story_path}} directly</action>
      <action>Read COMPLETE story file</action>
      <action>Extract story_key from filename or metadata</action>
      <goto anchor="task_check" />
    </check>

    <!-- Sprint-based story discovery -->
    <check if="{{sprint_status}} file exists">
      <critical>MUST read COMPLETE sprint-status.yaml file from start to end to preserve order</critical>
      <action>Load the FULL file: {{sprint_status}}</action>
      <action>Read ALL lines from beginning to end - do not skip any content</action>
      <action>Parse the development_status section completely to understand story order</action>

      <action>Find the FIRST story (by reading in order from top to bottom) where:
        - Key matches pattern: number-number-name (e.g., "1-2-user-auth")
        - NOT an epic key (epic-X) or retrospective (epic-X-retrospective)
        - Status value equals "ready-for-dev"
      </action>

      <check if="no ready-for-dev or in-progress story found">
        <output>üìã No ready-for-dev stories found in sprint-status.yaml

          **Current Sprint Status:** {{sprint_status_summary}}

          **What would you like to do?**
          1. Run `create-story` to create next story from epics with comprehensive context
          2. Run `*validate-create-story` to improve existing stories before development (recommended quality check)
          3. Specify a particular story file to develop (provide full path)
          4. Check {{sprint_status}} file to see current sprint status

          üí° **Tip:** Stories in `ready-for-dev` may not have been validated. Consider running `validate-create-story` first for a quality
          check.
        </output>
        <ask>Choose option [1], [2], [3], or [4], or specify story file path:</ask>

        <check if="user chooses '1'">
          <action>HALT - Run create-story to create next story</action>
        </check>

        <check if="user chooses '2'">
          <action>HALT - Run validate-create-story to improve existing stories</action>
        </check>

        <check if="user chooses '3'">
          <ask>Provide the story file path to develop:</ask>
          <action>Store user-provided story path as {{story_path}}</action>
          <goto anchor="task_check" />
        </check>

        <check if="user chooses '4'">
          <output>Loading {{sprint_status}} for detailed status review...</output>
          <action>Display detailed sprint status analysis</action>
          <action>HALT - User can review sprint status and provide story path</action>
        </check>

        <check if="user provides story file path">
          <action>Store user-provided story path as {{story_path}}</action>
          <goto anchor="task_check" />
        </check>
      </check>
    </check>

    <!-- Non-sprint story discovery -->
    <check if="{{sprint_status}} file does NOT exist">
      <action>Search {story_dir} for stories directly</action>
      <action>Find stories with "ready-for-dev" status in files</action>
      <action>Look for story files matching pattern: *-*-*.md</action>
      <action>Read each candidate story file to check Status section</action>

      <check if="no ready-for-dev stories found in story files">
        <output>üìã No ready-for-dev stories found

          **Available Options:**
          1. Run `create-story` to create next story from epics with comprehensive context
          2. Run `*validate-create-story` to improve existing stories
          3. Specify which story to develop
        </output>
        <ask>What would you like to do? Choose option [1], [2], or [3]:</ask>

        <check if="user chooses '1'">
          <action>HALT - Run create-story to create next story</action>
        </check>

        <check if="user chooses '2'">
          <action>HALT - Run validate-create-story to improve existing stories</action>
        </check>

        <check if="user chooses '3'">
          <ask>It's unclear what story you want developed. Please provide the full path to the story file:</ask>
          <action>Store user-provided story path as {{story_path}}</action>
          <action>Continue with provided story file</action>
        </check>
      </check>

      <check if="ready-for-dev story found in files">
        <action>Use discovered story file and extract story_key</action>
      </check>
    </check>

    <action>Store the found story_key (e.g., "1-2-user-authentication") for later status updates</action>
    <action>Find matching story file in {story_dir} using story_key pattern: {{story_key}}.md</action>
    <action>Read COMPLETE story file from discovered path</action>

    <anchor id="task_check" />

    <action>Parse sections: Story, Acceptance Criteria, Tasks/Subtasks, Dev Notes, Dev Agent Record, File List, Change Log, Status</action>

    <action>Load comprehensive context from story file's Dev Notes section</action>
    <action>Extract developer guidance from Dev Notes: architecture requirements, previous learnings, technical specifications</action>
    <action>Use enhanced story context to inform implementation decisions and approaches</action>

    <action>Identify first incomplete task (unchecked [ ]) in Tasks/Subtasks</action>

    <action if="no incomplete tasks">
      <goto step="6">Completion sequence</goto>
    </action>
    <action if="story file inaccessible">HALT: "Cannot develop story without access to story file"</action>
    <action if="incomplete task or subtask requirements ambiguous">ASK user to clarify or HALT</action>
  </step>

  <step n="2" goal="Load project context and story information">
    <critical>Load all available context to inform implementation</critical>

    <action>Load {project_context} for coding standards and project-wide patterns (if exists)</action>
    <action>Parse sections: Story, Acceptance Criteria, Tasks/Subtasks, Dev Notes, Dev Agent Record, File List, Change Log, Status</action>
    <action>Load comprehensive context from story file's Dev Notes section</action>
    <action>Extract developer guidance from Dev Notes: architecture requirements, previous learnings, technical specifications</action>
    <action>Use enhanced story context to inform implementation decisions and approaches</action>
    <output>‚úÖ **Context Loaded**
      Story and project context available for implementation
    </output>
  </step>

  <step n="3" goal="Detect review continuation and extract review context">
    <critical>Determine if this is a fresh start or continuation after code review</critical>

    <action>Check if "Senior Developer Review (AI)" section exists in the story file</action>
    <action>Check if "Review Follow-ups (AI)" subsection exists under Tasks/Subtasks</action>

    <check if="Senior Developer Review section exists">
      <action>Set review_continuation = true</action>
      <action>Extract from "Senior Developer Review (AI)" section:
        - Review outcome (Approve/Changes Requested/Blocked)
        - Review date
        - Total action items with checkboxes (count checked vs unchecked)
        - Severity breakdown (High/Med/Low counts)
      </action>
      <action>Count unchecked [ ] review follow-up tasks in "Review Follow-ups (AI)" subsection</action>
      <action>Store list of unchecked review items as {{pending_review_items}}</action>

      <output>‚èØÔ∏è **Resuming Story After Code Review** ({{review_date}})

        **Review Outcome:** {{review_outcome}}
        **Action Items:** {{unchecked_review_count}} remaining to address
        **Priorities:** {{high_count}} High, {{med_count}} Medium, {{low_count}} Low

        **Strategy:** Will prioritize review follow-up tasks (marked [AI-Review]) before continuing with regular tasks.
      </output>
    </check>

    <check if="Senior Developer Review section does NOT exist">
      <action>Set review_continuation = false</action>
      <action>Set {{pending_review_items}} = empty</action>

      <output>üöÄ **Starting Fresh Implementation**

        Story: {{story_key}}
        Story Status: {{current_status}}
        First incomplete task: {{first_task_description}}
      </output>
    </check>
  </step>

  <step n="4" goal="Mark story in-progress" tag="sprint-status">
    <check if="{{sprint_status}} file exists">
      <action>Load the FULL file: {{sprint_status}}</action>
      <action>Read all development_status entries to find {{story_key}}</action>
      <action>Get current status value for development_status[{{story_key}}]</action>

      <check if="current status == 'ready-for-dev' OR review_continuation == true">
        <action>Update the story in the sprint status report to = "in-progress"</action>
        <output>üöÄ Starting work on story {{story_key}}
          Status updated: ready-for-dev ‚Üí in-progress
        </output>
      </check>

      <check if="current status == 'in-progress'">
        <output>‚èØÔ∏è Resuming work on story {{story_key}}
          Story is already marked in-progress
        </output>
      </check>

      <check if="current status is neither ready-for-dev nor in-progress">
        <output>‚ö†Ô∏è Unexpected story status: {{current_status}}
          Expected ready-for-dev or in-progress. Continuing anyway...
        </output>
      </check>

      <action>Store {{current_sprint_status}} for later use</action>
    </check>

    <check if="{{sprint_status}} file does NOT exist">
      <output>‚ÑπÔ∏è No sprint status file exists - story progress will be tracked in story file only</output>
      <action>Set {{current_sprint_status}} = "no-sprint-tracking"</action>
    </check>
  </step>

  <step n="5" goal="Autonomous implementation using self-sufficient story">
    <critical>ü§ñ AUTONOMOUS DEVELOPMENT LOOP - Spawn subagent that works until 100% complete!</critical>
    <critical>üìã SELF-SUFFICIENT STORY - Agent needs ONLY story file + codebase, no external context!</critical>

    <check if="{task_tool_available} AND {subagents.enabled} AND {subagents.mode} != 'explicit'">
      <output>üöÄ Spawning autonomous development agent for story {{story_key}}...</output>
      <output>Agent will work continuously in loops until ALL tasks complete, ALL tests pass, ALL ACs met.</output>

      <invoke-subagent type="general" mode="blocking">
        <objective>
          Autonomously implement story {{story_key}} to 100% completion:

          **Phase 1: Understand**
          - Read COMPLETE story file at {{story_file}} (self-sufficient with embedded context)
          - Understand ALL acceptance criteria
          - Review ALL tasks/subtasks that need implementation
          - Review Dev Notes for architecture requirements, patterns, previous learnings
          - Explore existing codebase for patterns and conventions

          **Phase 2: Execute Tasks in Continuous Loop**
          - FOR EACH unchecked task in Tasks/Subtasks section (in order):
            1. Plan implementation approach based on Dev Notes guidance
            2. Write FAILING tests first (RED phase of TDD)
            3. Confirm tests fail (validates test correctness)
            4. Implement MINIMAL code to make tests pass (GREEN phase)
            5. Run full test suite to verify tests pass and no regressions
            6. IF tests fail: Debug and fix (max 3 attempts per task)
               - Retry 1: Analyze failure, adjust implementation
               - Retry 2: Review Dev Notes for missed requirements
               - Retry 3: Check for architecture violations
               - IF still failing after 3 attempts: HALT with detailed error
            7. Refactor code while keeping tests green (REFACTOR phase)
            8. Validate task meets related acceptance criteria
            9. ONLY THEN mark task checkbox [x] in story file
            10. Update File List with ALL new/modified/deleted files
            11. Add implementation notes to Dev Agent Record
            12. Save story file
            13. Continue to NEXT unchecked task

          - CONTINUE this loop WITHOUT STOPPING until ALL tasks are checked

          **Phase 3: Final Validation**
          - Verify ALL tasks/subtasks marked [x]
          - Run full regression test suite - ALL must pass
          - Validate ALL acceptance criteria satisfied
          - Verify File List includes every changed file
          - Update story Status to "review"
          - Add final completion notes to Dev Agent Record

          **Phase 4: Report Results**
          - Return completion status: "completed" or "halted"
          - Return test results summary (total, passing, failing)
          - Return list of files modified
          - Return any blocking issues encountered

          **HALT Conditions (only stop for these):**
          - 3 consecutive test failures on same task (needs human debugging)
          - Missing dependencies not specified in story Dev Notes
          - Ambiguous requirements in story file (needs clarification)
          - Configuration files missing that are required
          - Cannot determine how to run tests after exploration

          **Critical Requirements:**
          - Use ONLY story file + existing codebase (story is self-sufficient)
          - Follow TDD red-green-refactor cycle for EVERY task
          - Run tests after EVERY implementation change
          - NEVER mark task complete unless tests 100% passing
          - NEVER skip tasks or change task order
          - Execute continuously without pausing until done or HALT
        </objective>
        <context>
          <story-file>{{story_file}}</story-file>
          <project-root>{{project_root}}</project-root>
          <story-dir>{{story_dir}}</story-dir>
        </context>
        <constraints>
          <max-turns>100</max-turns>
          <halt-on>
            - 3 consecutive test failures on same task
            - Missing dependencies not in story
            - Ambiguous requirements
            - Missing required configuration
          </halt-on>
        </constraints>
        <output-var>{autonomous_dev_results}</output-var>
      </invoke-subagent>

      <output>‚úÖ Autonomous development agent completed!</output>

      <!-- Process results -->
      <check if="{autonomous_dev_results}.status == 'completed'">
        <output>üéâ Story {{story_key}} fully implemented autonomously!</output>
        <output>Test Results: {autonomous_dev_results}.test_summary</output>
        <output>Files Modified: {autonomous_dev_results}.files_modified</output>
        <action>Proceed to step 9 for final validation</action>
      </check>

      <check if="{autonomous_dev_results}.status == 'halted'">
        <output>‚ö†Ô∏è Autonomous agent halted due to blocking issue:</output>
        <output>{autonomous_dev_results}.halt_reason</output>
        <output>Partial Progress:</output>
        <output>- Tasks completed: {autonomous_dev_results}.tasks_completed</output>
        <output>- Tasks remaining: {autonomous_dev_results}.tasks_remaining</output>
        <output>- Files modified so far: {autonomous_dev_results}.files_modified</output>
        <ask>How would you like to proceed?
          [r] Resume manually from where agent stopped
          [c] Clarify requirements and restart agent
          [h] Get human developer assistance
        </ask>
        <action>Based on user choice, either HALT or continue with manual intervention</action>
      </check>

      <check if="{autonomous_dev_results}.status == 'failed'">
        <output>‚ùå Autonomous agent encountered fatal error: {autonomous_dev_results}.error</output>
        <action>Fall back to manual implementation loop below</action>
      </check>
    </check>

    <!-- Fallback: Manual implementation if Task tool unavailable or explicit mode -->
    <else>
      <output>‚ÑπÔ∏è Using manual implementation mode (autonomous agents disabled or unavailable)</output>

      <!-- Original manual implementation logic -->
      <critical>FOLLOW THE STORY FILE TASKS/SUBTASKS SEQUENCE EXACTLY AS WRITTEN - NO DEVIATION</critical>

      <action>Review the current task/subtask from the story file - this is your authoritative implementation guide</action>
      <action>Plan implementation following red-green-refactor cycle</action>

      <!-- RED PHASE -->
      <action>Write FAILING tests first for the task/subtask functionality</action>
      <action>Confirm tests fail before implementation - this validates test correctness</action>

      <!-- GREEN PHASE -->
      <action>Implement MINIMAL code to make tests pass</action>
      <action>Run tests to confirm they now pass</action>
      <action>Handle error conditions and edge cases as specified in task/subtask</action>

      <!-- REFACTOR PHASE -->
      <action>Improve code structure while keeping tests green</action>
      <action>Ensure code follows architecture patterns and coding standards from Dev Notes</action>

      <action>Document technical approach and decisions in Dev Agent Record ‚Üí Implementation Plan</action>

      <action if="new dependencies required beyond story specifications">HALT: "Additional dependencies need user approval"</action>
      <action if="3 consecutive implementation failures occur">HALT and request guidance</action>
      <action if="required configuration is missing">HALT: "Cannot proceed without necessary configuration files"</action>

      <critical>NEVER implement anything not mapped to a specific task/subtask in the story file</critical>
      <critical>NEVER proceed to next task until current task/subtask is complete AND tests pass</critical>
      <critical>Execute continuously without pausing until all tasks/subtasks are complete or explicit HALT condition</critical>

      <!-- Manual task validation and loop -->
      <action>Verify ALL tests for this task/subtask ACTUALLY EXIST and PASS 100%</action>
      <action>Confirm implementation matches EXACTLY what the task/subtask specifies</action>
      <action>Validate that ALL acceptance criteria related to this task are satisfied</action>
      <action>Run full test suite to ensure NO regressions introduced</action>

      <check if="ALL validation gates pass AND tests ACTUALLY exist and pass">
        <action>Mark the task (and subtasks) checkbox with [x]</action>
        <action>Update File List section with ALL new, modified, or deleted files</action>
        <action>Add completion notes to Dev Agent Record</action>
      </check>

      <check if="ANY validation fails">
        <action>DO NOT mark task complete - fix issues first</action>
        <action>HALT if unable to fix validation failures</action>
      </check>

      <action>Save the story file</action>
      <action>Determine if more incomplete tasks remain</action>
      <action if="more tasks remain">
        <goto step="5">Next task</goto>
      </action>
      <action if="no tasks remain">
        <goto step="9">Completion</goto>
      </action>
    </else>
  </step>

  <step n="9" goal="Story completion and mark for review" tag="sprint-status">
    <critical>üîç FINAL VALIDATION - Ensure 100% completion before marking for review</critical>

    <!-- Read current story file to verify state (autonomous agent may have updated it) -->
    <action>Re-read complete story file: {{story_file}}</action>
    <action>Verify ALL tasks and subtasks are marked [x]</action>
    <action>Verify story Status is "review" (autonomous agent should have set this)</action>

    <!-- Validate test results -->
    <check if="autonomous agent was used">
      <action>Review test results from {autonomous_dev_results}.test_summary</action>
      <check if="{autonomous_dev_results}.all_tests_passing != true">
        <output>‚ùå HALT: Tests not all passing!</output>
        <output>Test Summary: {autonomous_dev_results}.test_summary</output>
        <action>HALT - Cannot mark for review with failing tests</action>
      </check>
    </check>
    <else>
      <action>Run the full regression suite manually (do not skip)</action>
      <check if="any tests fail">
        <output>‚ùå HALT: Tests failing!</output>
        <action>HALT - Fix failing tests before marking for review</action>
      </check>
    </else>

    <action>Confirm File List includes every changed file</action>
    <action>Execute enhanced definition-of-done validation</action>

    <!-- Enhanced Definition of Done Validation -->
    <action>Validate definition-of-done checklist with essential requirements:
      - All tasks/subtasks marked complete with [x]
      - Implementation satisfies every Acceptance Criterion
      - Unit tests for core functionality added/updated
      - Integration tests for component interactions added when required
      - End-to-end tests for critical flows added when story demands them
      - All tests pass 100% (no regressions, new tests successful)
      - Code quality checks pass (linting, static analysis if configured)
      - File List includes every new/modified/deleted file (relative paths)
      - Dev Agent Record contains implementation notes
      - Change Log includes summary of changes (or Dev Agent Record has completion notes)
      - Only permitted story sections were modified
      - Story uses ONLY embedded context (no external file dependencies)
    </action>

    <!-- Update story status if not already done by autonomous agent -->
    <check if="story Status != 'review'">
      <action>Update the story Status to: "review"</action>
    </check>

    <!-- Mark story ready for review - sprint status conditional -->
    <check if="{sprint_status} file exists AND {{current_sprint_status}} != 'no-sprint-tracking'">
      <action>Load the FULL file: {sprint_status}</action>
      <action>Find development_status key matching {{story_key}}</action>
      <action>Verify current status is "in-progress" (expected previous state)</action>
      <action>Update development_status[{{story_key}}] = "review"</action>
      <action>Save file, preserving ALL comments and structure including STATUS DEFINITIONS</action>
      <output>‚úÖ Story status updated to "review" in sprint-status.yaml</output>
    </check>

    <check if="{sprint_status} file does NOT exist OR {{current_sprint_status}} == 'no-sprint-tracking'">
      <output>‚ÑπÔ∏è Story status updated to "review" in story file (no sprint tracking configured)</output>
    </check>

    <check if="story key not found in sprint status">
      <output>‚ö†Ô∏è Story file updated, but sprint-status update failed: {{story_key}} not found

        Story status is set to "review" in file, but sprint-status.yaml may be out of sync.
      </output>
    </check>

    <!-- Final validation gates -->
    <action if="any task is incomplete">HALT - Complete remaining tasks before marking ready for review</action>
    <action if="regression failures exist">HALT - Fix regression issues before completing</action>
    <action if="File List is incomplete">HALT - Update File List with all changed files</action>
    <action if="definition-of-done validation fails">HALT - Address DoD failures before completing</action>
  </step>

  <step n="10" goal="Completion communication and user support">
    <action>Execute the enhanced definition-of-done checklist using the validation framework</action>

    <check if="autonomous agent was used">
      <output>**ü§ñ AUTONOMOUS DEVELOPMENT COMPLETED, {user_name}!**

        **Story Details:**
        - Story ID: {{story_id}}
        - Story Key: {{story_key}}
        - File: {{story_file}}
        - Status: review (ready for code review)

        **Autonomous Execution Summary:**
        - Mode: Autonomous loop execution (zero human intervention)
        - Tasks completed: {autonomous_dev_results}.tasks_completed
        - Test results: {autonomous_dev_results}.test_summary
        - Files modified: {autonomous_dev_results}.files_count
        - Execution approach: TDD red-green-refactor for all tasks
        - Retries/failures handled: {autonomous_dev_results}.retry_count

        **Key Accomplishments:**
        {autonomous_dev_results}.key_changes

        **Files Modified:**
        {autonomous_dev_results}.files_modified

        **Next Steps:**
        1. Review the implemented story in {{story_file}}
        2. Verify all acceptance criteria are met (autonomous agent validated, but double-check!)
        3. Run `code-review` workflow for comprehensive review
        4. Optional: Run `/bmad:tea:automate` for additional test coverage
        5. Once approved, story can be merged

        üí° **Tip:** For best results, run `code-review` using a **different** LLM than the one that implemented.
      </output>
    </check>
    <else>
      <output>**‚úÖ STORY IMPLEMENTATION COMPLETED, {user_name}!**

        **Story Details:**
        - Story ID: {{story_id}}
        - Story Key: {{story_key}}
        - File: {{story_file}}
        - Status: review (ready for code review)

        **Implementation Summary:**
        - Tasks completed: ALL
        - Tests: ALL passing
        - Files modified: See File List in story file

        **Next Steps:**
        1. Review the implemented story in {{story_file}}
        2. Verify all acceptance criteria are met
        3. Run `code-review` workflow for peer review
        4. Optional: Run `/bmad:tea:automate` to expand test coverage

        üí° **Tip:** For best results, run `code-review` using a **different** LLM.
      </output>
    </else>

    <action>Based on {user_skill_level}, ask if user needs any explanations about:
      - What was implemented and how it works
      - Why certain technical decisions were made
      - How to test or verify the changes
      - Any patterns, libraries, or approaches used
      - Anything else they'd like clarified
    </action>

    <check if="user asks for explanations">
      <action>Provide clear, contextual explanations tailored to {user_skill_level}</action>
      <action>Use examples and references to specific code when helpful</action>
    </check>

    <check if="{sprint_status} file exists">
      <action>Suggest checking {sprint_status} to see overall project progress</action>
    </check>
    <action>Remain flexible - allow user to choose their own path or ask for other assistance</action>
  </step>

</workflow>
